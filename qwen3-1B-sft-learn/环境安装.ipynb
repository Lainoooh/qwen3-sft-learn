{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2568669c-c126-4791-93f8-d070a5835bcf",
   "metadata": {},
   "source": [
    "# 安装步骤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27090561-03ac-471e-acb0-c7dbeb373e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib2to3.pgen2.grammar import line\n",
    "\n",
    "linux控制台上使用root安装登录jupyter\n",
    "\n",
    "\n",
    "# 更新apt源，输入你的sudo密码\n",
    "sudo apt update && sudo apt upgrade -y\n",
    "\n",
    "# 安装Python3、pip3和必要依赖（Ubuntu默认没有预装pip）\n",
    "sudo apt install -y python3 python3-pip python3-dev\n",
    "\n",
    "# 升级pip3，加--user避免权限问题\n",
    "pip3 install --upgrade pip\n",
    "\n",
    "# 安装JupyterLab\n",
    "pip3 install jupyterlab\n",
    "# （可选）如果需要支持扩展插件，安装扩展管理器\n",
    "pip3 install jupyterlab_widgets\n",
    "\n",
    "jupyter lab --version\n",
    "\n",
    "# 1. 获取 jupyter 可执行文件路径\n",
    "which jupyter\n",
    "# 输出示例：/home/你的用户名/.local/bin/jupyter\n",
    "\n",
    "#2.创建并编辑服务文件：\n",
    "sudo vim /etc/systemd/system/jupyter-lab.service\n",
    "<<<\n",
    "[Unit]\n",
    "Description=JupyterLab Service (Root User)\n",
    "After=network.target\n",
    "\n",
    "[Service]\n",
    "Type=simple\n",
    "User=root\n",
    "ExecStart=/usr/local/bin/jupyter lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root\n",
    "WorkingDirectory=/home/wanglei/pyenv\n",
    "Restart=always\n",
    "RestartSec=10\n",
    "StandardOutput=append:/var/log/jupyter-lab.log\n",
    "StandardError=append:/var/log/jupyter-lab.err.log\n",
    "LimitNOFILE=65535\n",
    "\n",
    "[Install]\n",
    "WantedBy=multi-user.target\n",
    ">>>\n",
    "\n",
    "# 重载systemd配置（让修改生效）\n",
    "sudo systemctl daemon-reload\n",
    "\n",
    "# 重启服务\n",
    "sudo systemctl restart jupyter-lab.service\n",
    "\n",
    "# 查看服务状态（此时应显示active (running)）\n",
    "sudo systemctl status jupyter-lab.service\n",
    "\n",
    "# 检查8888端口\n",
    "sudo netstat -tlnp | grep 8888\n",
    "\n",
    "#执行密码设置命令\n",
    "# 用root权限执行（和服务运行用户一致）\n",
    "sudo jupyter server password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7137128b-9a21-4b60-8f02-ab94cd91a00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#一、全量卸载（系统级 + Python 包，彻底清空）\n",
    "#1. 卸载系统级 CUDA Toolkit（若已安装，全量清理）\n",
    "#适用于 Linux 系统（Ubuntu/CentOS 通用），彻底卸载所有 CUDA 版本：\n",
    "\n",
    "# 停止CUDA相关服务（如有）\n",
    "sudo systemctl stop nvidia-persistenced 2>/dev/null\n",
    "\n",
    "# 卸载CUDA Toolkit（覆盖12.x所有版本）\n",
    "sudo apt-get purge -y nvidia* cuda* cudnn* 2>/dev/null\n",
    "sudo apt-get autoremove -y\n",
    "sudo rm -rf /usr/local/cuda* /usr/lib/cuda* /usr/include/cuda*\n",
    "sudo rm -rf ~/.nv ~/.cuda\n",
    "\n",
    "# 清理CUDA环境变量（避免残留）\n",
    "sed -i '/CUDA_HOME/d' ~/.bashrc\n",
    "sed -i '/cuda/d' ~/.bashrc\n",
    "sed -i '/NVIDIA/d' ~/.bashrc\n",
    "source ~/.bashrc\n",
    "\n",
    "#2. 卸载所有 Python 依赖（全量清理）\n",
    "pip3 uninstall -y \\\n",
    "    torch torchvision torchaudio \\\n",
    "    transformers auto-gptq optimum peft \\\n",
    "    modelscope pyarrow numpy bitsandbytes triton datasets \\\n",
    "    accelerate sentencepiece safetensors\n",
    "\n",
    "# 清理pip缓存+残留文件\n",
    "pip3 cache purge\n",
    "rm -rf ~/.cache/huggingface ~/.cache/modelscope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3316012-ce58-4b0b-bca2-024b8490c5af",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#步骤 1：安装适配的 CUDA Toolkit 12.1\n",
    "#####cuda版本需注意安装12.1版本，按照你的显卡、cuda实际版本参考，尽量选择稳定版本\n",
    "nvidia-smi #查看当前硬件版本和cuda驱动\n",
    "nvcc -V  # 输出CUDA Version 12.1即正确\n",
    "\n",
    "# 因我的环境是cuda是12.8，在选择量化时报错bitsandbytes 预编译包、triton 依赖不兼容，所以需要卸载CUDA12.8，换成12.1\n",
    "# 1. 安装CUDA 12.1依赖\n",
    "sudo apt-get update && sudo apt-get install -y \\\n",
    "    gcc g++ make linux-headers-$(uname -r)\n",
    "\n",
    "# 2. 下载并安装CUDA 12.1（Ubuntu 20.04/22.04通用）\n",
    "wget https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda_12.1.0_530.30.02_linux.run\n",
    "sudo sh cuda_12.1.0_530.30.02_linux.run --silent --toolkit\n",
    "rm -f cuda_12.1.0_530.30.02_linux.run\n",
    "\n",
    "# 3. 配置CUDA环境变量（生效当前会话+永久生效）\n",
    "echo \"export CUDA_HOME=/usr/local/cuda-12.1\" >> ~/.bashrc\n",
    "echo \"export PATH=\\$CUDA_HOME/bin:\\$PATH\" >> ~/.bashrc\n",
    "echo \"export LD_LIBRARY_PATH=\\$CUDA_HOME/lib64:\\$LD_LIBRARY_PATH\" >> ~/.bashrc\n",
    "source ~/.bashrc\n",
    "\n",
    "# 验证CUDA安装\n",
    "nvcc -V  # 应输出 CUDA Version 12.1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d75e94-30b7-41cc-a7d0-f07e104a4184",
   "metadata": {},
   "outputs": [],
   "source": [
    "#步骤 2：安装 PyTorch（基础依赖，先装核心框架）\n",
    "#严格按官网量化要求选torch==2.1，且匹配 CUDA 12.1\n",
    "#####pytorch则需要按照cuda的版本进行选择安装，可参考官网 https://pytorch.org/get-started/locally/\n",
    "# CUDA 12.1\n",
    "pip install torch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "#此处安装可能会有警告，可以跳过"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aff5648-6819-4c7f-a6c1-895e47eec14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#步骤 3：安装量化相关依赖\n",
    "# 1. 先装基础依赖（numpy/pyarrow，避免量化包安装时依赖缺失）\n",
    "pip3 install numpy==1.26.4 pyarrow==14.0.1 sentencepiece safetensors\n",
    "\n",
    "# 2. 装官网要求的量化核心包（严格匹配版本）\n",
    "pip3 install \\\n",
    "transformers==4.34.1 \\\n",
    "auto-gptq==0.5.1 \\\n",
    "optimum==1.14.0 \\\n",
    "peft==0.6.2 \\\n",
    "modelscope==1.14.0 \\\n",
    "accelerate==0.24.1 \\\n",
    "huggingface_hub==0.19.4 \\\n",
    "deepspeed==0.14.0\n",
    "openmpi-bin openmpi-common libopenmpi-dev  mpi4py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "face27a3-c822-4d58-a8ba-794edd8924d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#linux控制台上qwen1.8B 安装\n",
    "#参考地址：https://github.com/QwenLM/Qwen/blob/main/README_CN.md\n",
    "\n",
    "# 1. 安装git（如果未安装）\n",
    "sudo apt install -y git\n",
    "\n",
    "# 2. 克隆Qwen官方仓库（包含requirements.txt）\n",
    "cd /home/wanglei/pyenv\n",
    "git clone https://github.com/QwenLM/Qwen.git\n",
    "cd Qwen/\n",
    "\n",
    "#优选：安装transformers指定版本：4.32.0 （需配套）\n",
    "/usr/bin/pip3 install transformers=4.32.0\n",
    "    \n",
    "#安装版本其他依赖\n",
    "pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d9eb8c-2ed5-4b42-94b0-83a2c8de0c0e",
   "metadata": {},
   "source": [
    "# 检查步骤 python代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a06a34f5-6fe7-499c-8353-1455efe7e629",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T01:29:41.042699269Z",
     "start_time": "2026-02-04T01:29:40.917708255Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.12\n"
     ]
    }
   ],
   "source": [
    "# 1. 检查Python版本\n",
    "!python3 --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa69ca65-aea2-4249-b84c-9df506a15867",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-25T05:27:45.691967067Z",
     "start_time": "2026-01-25T05:27:45.646420794Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter内核Python版本：3.10.12\n"
     ]
    }
   ],
   "source": [
    "# 2. 检查Jupyter内核Python版本\n",
    "import sys\n",
    "print(f\"Jupyter内核Python版本：{sys.version.split()[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff09e7f9-0a8e-4da3-b278-b0053d2cbad9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-25T05:27:49.222844778Z",
     "start_time": "2026-01-25T05:27:48.218805630Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch版本：2.1.2+cu121\n"
     ]
    }
   ],
   "source": [
    "# 3. 检查PyTorch版本\n",
    "import torch\n",
    "print(f\"PyTorch版本：{torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e6008ea-a2d0-471f-a0a5-332193cceb7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-25T05:27:55.227602193Z",
     "start_time": "2026-01-25T05:27:55.002761287Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers版本：4.34.1\n"
     ]
    }
   ],
   "source": [
    "# 4. 检查Transformers版本\n",
    "import transformers\n",
    "print(f\"Transformers版本：{transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de436c6c-fcfd-45c4-a50a-c4d7b2edc65c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-25T05:27:57.800540417Z",
     "start_time": "2026-01-25T05:27:57.739827893Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA是否可用：True\n",
      "CUDA版本（PyTorch检测）：12.1\n"
     ]
    }
   ],
   "source": [
    "# 5. 检查CUDA是否可用\n",
    "import torch\n",
    "print(f\"CUDA是否可用：{torch.cuda.is_available()}\")\n",
    "print(f\"CUDA版本（PyTorch检测）：{torch.version.cuda if torch.cuda.is_available() else '未安装CUDA'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f9c0dcc-36ca-42cc-8351-610da9ee3dad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-25T05:28:02.444987807Z",
     "start_time": "2026-01-25T05:28:01.824508402Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: nvcc: not found\n",
      "Wed Feb  4 09:38:28 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.172.18             Driver Version: 570.172.18     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX5880-Ada-8Q          Off |   00000000:00:09.0 Off |                  N/A |\n",
      "| N/A   N/A    P0            N/A  /  N/A  |    2387MiB /   8192MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A            1608      G   /usr/bin/gnome-shell                    312MiB |\n",
      "|    0   N/A  N/A            2351      G   ...ersion=20260203-010043.952000        821MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# 6. 检查系统CUDA版本\n",
    "!nvcc -V  # 查看已安装的CUDA Toolkit版本\n",
    "!nvidia-smi  # 查看显卡驱动支持的最高CUDA版本"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c839902e-bdc2-454b-bf77-9fb8652dcdaa",
   "metadata": {},
   "source": [
    "# 加载 Qwen-1.8B-Chat-Int4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9757726d-c40b-414a-b369-7ffd9223b6c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-25T06:46:15.495820637Z",
     "start_time": "2026-01-25T06:41:01.490920547Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-04 09:38:42,058 - modelscope - INFO - PyTorch version 2.1.2+cu121 Found.\n",
      "2026-02-04 09:38:42,059 - modelscope - INFO - Loading ast index from /root/.cache/modelscope/ast_indexer\n",
      "2026-02-04 09:38:42,089 - modelscope - INFO - Loading done! Current index file version is 1.14.0, with md5 dc2d27292ce4d5b00f912562a91fbfec and a total number of 976 components indexed\n",
      "2026-02-04 09:38:42,877 - modelscope - INFO - Use user-specified model revision: v1.0.0\n",
      "Try importing flash-attention for faster inference...\n",
      "Warning: import flash_attn rotary fail, please install FlashAttention rotary to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary\n",
      "Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm\n",
      "Warning: import flash_attn fail, please install FlashAttention to get higher efficiency https://github.com/Dao-AILab/flash-attention\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 模型回复：您好！我是来自阿里云的大规模语言模型，我叫通义千问。作为阿里云的一员，我的主要功能是为用户提供自然语言处理服务。我可以回答问题、创作文字，还能表达观点、撰写代码。如果您有任何关于人工智能或者自然语言处理的问题，欢迎随时向我提问哦！\n",
      "✅ 模型运行设备：cuda:0\n"
     ]
    }
   ],
   "source": [
    "from modelscope import snapshot_download\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# 1. 下载量化模型\n",
    "model_dir = snapshot_download(\n",
    "    'qwen/Qwen-1_8B-Chat-Int4',\n",
    "    revision='v1.0.0',\n",
    "    cache_dir='../qwen_model'\n",
    ")\n",
    "\n",
    "# 2. 加载tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\n",
    "\n",
    "# 3. 关键：AutoModelForCausalLM加载量化模型（依赖AutoGPTQ）\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_dir,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ").eval()\n",
    "\n",
    "# 4. 测试推理\n",
    "response, history = model.chat(tokenizer, \"你好，请介绍一下自己\", history=None)\n",
    "print(f\"✅ 模型回复：{response}\")\n",
    "print(f\"✅ 模型运行设备：{next(model.parameters()).device}\")  # 输出cuda:0即GPU生效\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72106daa4b9ad056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a6a5cd0c1e09b8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-25T06:17:20.501698770Z",
     "start_time": "2026-01-25T06:17:20.132910799Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练数据生成完成！文件路径：train.json\n",
      "共生成 1000 条训练样本\n",
      "\n",
      "第一条样本示例：\n",
      "{\n",
      "  \"conversations\": [\n",
      "    {\n",
      "      \"from\": \"user\",\n",
      "      \"value\": \"\\n给定一句话：“12月12日镇宁布依族苗族自治县的天气”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\\\"city\\\":城市,\\\"date\\\":日期}\\n\\n请问，这个JSON字符串是：\\n\"\n",
      "    },\n",
      "    {\n",
      "      \"from\": \"assistant\",\n",
      "      \"value\": \"{\\\"city\\\": \\\"镇宁布依族苗族自治县\\\", \\\"date\\\": \\\"12-12\\\"}\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import json\n",
    "import time\n",
    "\n",
    "with open('./city.txt', 'r', encoding='utf-8') as f:\n",
    "    city_list = f.readlines()\n",
    "    city_list = [line.strip().split(' ')[1] for line in city_list]\n",
    "\n",
    "Q='青岛4月6日下雨么?'\n",
    "\n",
    "prompt_template='''\n",
    "给定一句话：“%s”，请你按步骤要求工作。\n",
    "\n",
    "步骤1：识别这句话中的城市和日期共2个信息\n",
    "步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\n",
    "\n",
    "请问，这个JSON字符串是：\n",
    "'''\n",
    "\n",
    "\n",
    "Q_arr = []\n",
    "A_arr = []\n",
    "\n",
    "Q_list = [\n",
    "    ('{city}{year}年{month}月{day}日的天气','%Y-%m-%d'),\n",
    "    ('{city}{year}年{month}月{day}号的天气','%Y-%m-%d'),\n",
    "    ('{city}{month}月{day}日的天气','%m-%d'),\n",
    "    ('{city}{month}月{day}号的天气','%m-%d'),\n",
    "\n",
    "    ('{year}年{month}月{day}日{city}的天气','%Y-%m-%d'),\n",
    "    ('{year}年{month}月{day}号{city}的天气','%Y-%m-%d'),\n",
    "    ('{month}月{day}日{city}的天气','%m-%d'),\n",
    "    ('{month}月{day}号{city}的天气','%m-%d'),\n",
    "\n",
    "    ('你们{year}年{month}月{day}日去{city}玩吗？','%Y-%m-%d'),\n",
    "    ('你们{year}年{month}月{day}号去{city}玩么？','%Y-%m-%d'),\n",
    "    ('你们{month}月{day}日去{city}玩吗？','%m-%d'),\n",
    "    ('你们{month}月{day}号去{city}玩吗？','%m-%d'),\n",
    "]\n",
    "\n",
    "# 生成1000条训练样本（日期转换/天气相关）\n",
    "for i in range(1000):\n",
    "    Q_template = Q_list[random.randint(0, len(Q_list)-1)]\n",
    "    city = city_list[random.randint(0, len(city_list)-1)]\n",
    "    year = random.randint(1990, 2025)\n",
    "    month = random.randint(1, 12)\n",
    "    day = random.randint(1, 28)\n",
    "    time_str = '{}-{}-{}'.format(year, month, day)\n",
    "    date_field = time.strftime(Q_template[1], time.strptime(time_str, '%Y-%m-%d'))\n",
    "    question = Q_template[0].format(city=city, year=year, month=month, day=day)  # 原始问题\n",
    "    answer = json.dumps({'city': city, 'date': date_field}, ensure_ascii=False)  # 回答\n",
    "\n",
    "    Q_arr.append(prompt_template % question)\n",
    "    A_arr.append(answer)\n",
    "\n",
    "# ========== 核心修改：生成conversations格式 ==========\n",
    "train_data = []\n",
    "for q, a in zip(Q_arr, A_arr):\n",
    "    train_data.append({\n",
    "        \"conversations\": [\n",
    "            {\"from\": \"user\", \"value\": q},       # 用户问题\n",
    "            {\"from\": \"assistant\", \"value\": a}   # 助手回答\n",
    "        ]\n",
    "    })\n",
    "\n",
    "# ========== 写入JSON文件（UTF-8编码，适配finetune脚本） ==========\n",
    "output_json_path = \"train.json\"  # 输出的训练文件路径\n",
    "with open(output_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(train_data, f, ensure_ascii=False, indent=2)  # indent=2方便查看，不影响训练\n",
    "\n",
    "print(f\"训练数据生成完成！文件路径：{output_json_path}\")\n",
    "print(f\"共生成 {len(train_data)} 条训练样本\")\n",
    "# 打印第一条样本示例，方便你核对格式\n",
    "print(\"\\n第一条样本示例：\")\n",
    "print(json.dumps(train_data[0], ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9948195af6819e84",
   "metadata": {},
   "source": [
    "# 微调模型，生成到output_qwen\n",
    "\n",
    "使用qwen提供的finetune脚本进行lora微调\n",
    "qlora支持量化微调\n",
    "ds 全参微调\n",
    "single 单卡\n",
    "\n",
    "脚本执行：\n",
    "Usage: bash finetune/finetune_qlora_ds.sh [-m MODEL_PATH] [-d DATA_PATH]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fbac663f-035e-4c1f-abc6-4d76d1aa7405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Feb  4 09:49:04 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.172.18             Driver Version: 570.172.18     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX5880-Ada-8Q          Off |   00000000:00:09.0 Off |                  N/A |\n",
      "| N/A   N/A    P0            N/A  /  N/A  |    4961MiB /   8192MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A            1608      G   /usr/bin/gnome-shell                    322MiB |\n",
      "|    0   N/A  N/A            2351      G   ...ersion=20260203-010043.952000        738MiB |\n",
      "|    0   N/A  N/A            3472      C   /usr/bin/python3                       2692MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# 清空所有GPU的PyTorch显存缓存\n",
    "torch.cuda.empty_cache()\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "231df1f30de5d19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/wanglei/pyenv/Qwen-main\n",
      "/home/wanglei/pyenv/Qwen-main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "[2026-02-04 09:50:28,646] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "You passed `quantization_config` to `from_pretrained` but the model you're loading already has a `quantization_config` attribute and has already quantized weights. However, loading attributes (e.g. disable_exllama, use_cuda_fp16, max_input_length) will be overwritten with the one you passed to `from_pretrained`. The rest will be ignored.\n",
      "Try importing flash-attention for faster inference...\n",
      "Warning: import flash_attn rotary fail, please install FlashAttention rotary to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary\n",
      "Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm\n",
      "Warning: import flash_attn fail, please install FlashAttention to get higher efficiency https://github.com/Dao-AILab/flash-attention\n",
      "trainable params: 53,673,984 || all params: 676,104,192 || trainable%: 7.938714866006925\n",
      "Loading data...\n",
      "Formatting inputs...Skip in lazy mode\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:28: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import packaging  # type: ignore[attr-defined]\n",
      "Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "[1/3] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/usr/local/lib/python3.10/dist-packages/deepspeed/ops/csrc/includes -I/usr/local/lib/python3.10/dist-packages/deepspeed/ops/csrc/adam -isystem /usr/local/lib/python3.10/dist-packages/torch/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.10/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_89,code=compute_89 -gencode=arch=compute_89,code=sm_89 --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -lineinfo --use_fast_math -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_89,code=compute_89 -DBF16_AVAILABLE -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -std=c++17 -c /usr/local/lib/python3.10/dist-packages/deepspeed/ops/csrc/adam/multi_tensor_adam.cu -o multi_tensor_adam.cuda.o \n",
      "[2/3] c++ -MMD -MF fused_adam_frontend.o.d -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/usr/local/lib/python3.10/dist-packages/deepspeed/ops/csrc/includes -I/usr/local/lib/python3.10/dist-packages/deepspeed/ops/csrc/adam -isystem /usr/local/lib/python3.10/dist-packages/torch/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.10/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.10/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DBF16_AVAILABLE -c /usr/local/lib/python3.10/dist-packages/deepspeed/ops/csrc/adam/fused_adam_frontend.cpp -o fused_adam_frontend.o \n",
      "[3/3] c++ fused_adam_frontend.o multi_tensor_adam.cuda.o -shared -L/usr/local/lib/python3.10/dist-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o fused_adam.so\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 19.639922857284546 seconds\n",
      "  0%|                                                  | 0/1000 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "  0%|                                          | 1/1000 [00:01<23:33,  1.42s/it]tried to get lr value before scheduler/optimizer started stepping, returning lr=0\n",
      "{'loss': 0.5703, 'learning_rate': 0, 'epoch': 0.0}                              \n",
      "  0%|                                          | 2/1000 [00:02<19:36,  1.18s/it]tried to get lr value before scheduler/optimizer started stepping, returning lr=0\n",
      "{'loss': 0.3252, 'learning_rate': 0, 'epoch': 0.0}                              \n",
      "{'loss': 0.541, 'learning_rate': 0.0, 'epoch': 0.01}                            \n",
      "{'loss': 0.1509, 'learning_rate': 9.030899869919434e-05, 'epoch': 0.01}         \n",
      "{'loss': 0.3596, 'learning_rate': 0.0001431363764158987, 'epoch': 0.01}         \n",
      "{'loss': 0.108, 'learning_rate': 0.00018061799739838867, 'epoch': 0.01}         \n",
      "{'loss': 0.0338, 'learning_rate': 0.00020969100130080557, 'epoch': 0.01}        \n",
      "{'loss': 0.01, 'learning_rate': 0.00023344537511509303, 'epoch': 0.02}          \n",
      "{'loss': 0.0586, 'learning_rate': 0.000253529412004277, 'epoch': 0.02}          \n",
      "{'loss': 0.0452, 'learning_rate': 0.00027092699609758294, 'epoch': 0.02}        \n",
      "{'loss': 0.0639, 'learning_rate': 0.0002862727528317974, 'epoch': 0.02}         \n",
      "{'loss': 0.0345, 'learning_rate': 0.0002999999999999999, 'epoch': 0.02}         \n",
      "{'loss': 0.0284, 'learning_rate': 0.0003, 'epoch': 0.03}                        \n",
      "{'loss': 0.0042, 'learning_rate': 0.0003, 'epoch': 0.03}                        \n",
      "{'loss': 0.043, 'learning_rate': 0.0003, 'epoch': 0.03}                         \n",
      "{'loss': 0.1033, 'learning_rate': 0.0003, 'epoch': 0.03}                        \n",
      "{'loss': 0.0296, 'learning_rate': 0.0003, 'epoch': 0.03}                        \n",
      "{'loss': 0.0109, 'learning_rate': 0.0003, 'epoch': 0.04}                        \n",
      "{'loss': 0.0033, 'learning_rate': 0.0003, 'epoch': 0.04}                        \n",
      "{'loss': 0.0226, 'learning_rate': 0.0003, 'epoch': 0.04}                        \n",
      "{'loss': 0.0054, 'learning_rate': 0.0003, 'epoch': 0.04}                        \n",
      "{'loss': 0.0014, 'learning_rate': 0.0003, 'epoch': 0.04}                        \n",
      "{'loss': 0.0227, 'learning_rate': 0.0003, 'epoch': 0.05}                        \n",
      "{'loss': 0.0565, 'learning_rate': 0.0003, 'epoch': 0.05}                        \n",
      "{'loss': 0.0232, 'learning_rate': 0.0003, 'epoch': 0.05}                        \n",
      "{'loss': 0.0009, 'learning_rate': 0.0003, 'epoch': 0.05}                        \n",
      "{'loss': 0.0007, 'learning_rate': 0.0003, 'epoch': 0.05}                        \n",
      "  3%|█                                        | 27/1000 [00:27<16:55,  1.04s/it]^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/wanglei/pyenv/Qwen-main/finetune.py\", line 374, in <module>\n",
      "    train()\n",
      "  File \"/home/wanglei/pyenv/Qwen-main/finetune.py\", line 367, in train\n",
      "    trainer.train()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1591, in train\n",
      "    return inner_training_loop(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1892, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2787, in training_step\n",
      "    self.accelerator.backward(loss)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\", line 1983, in backward\n",
      "    self.deepspeed_engine_wrapped.backward(loss, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/utils/deepspeed.py\", line 167, in backward\n",
      "    self.engine.backward(loss, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
      "    ret_val = func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/engine.py\", line 1976, in backward\n",
      "    self.optimizer.backward(loss, retain_graph=retain_graph)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/stage_1_and_2.py\", line 2051, in backward\n",
      "    self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/fp16/loss_scaler.py\", line 63, in backward\n",
      "    scaled_loss.backward(retain_graph=retain_graph)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\", line 492, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 251, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n",
      "  3%|█                                        | 27/1000 [00:28<17:08,  1.06s/it]\n"
     ]
    }
   ],
   "source": [
    "%cd /home/wanglei/pyenv/Qwen-main/\n",
    "!pwd\n",
    "!bash finetune/finetune_qlora_single_gpu.sh -m /home/wanglei/pyenv/qwen_model/qwen/Qwen-1_8B-Chat-Int4 -d /home/wanglei/pyenv/qwen3-1B-sft-learn/train.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818ab23b-55a7-4cb0-9e0c-cf78f14934ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载SFT后的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8b471f86-e107-4e88-8cc8-545574c6ac80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed `quantization_config` to `from_pretrained` but the model you're loading already has a `quantization_config` attribute and has already quantized weights. However, loading attributes (e.g. disable_exllama, use_cuda_fp16, max_input_length) will be overwritten with the one you passed to `from_pretrained`. The rest will be ignored.\n",
      "Try importing flash-attention for faster inference...\n",
      "Warning: import flash_attn rotary fail, please install FlashAttention rotary to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary\n",
      "Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm\n",
      "Warning: import flash_attn fail, please install FlashAttention to get higher efficiency https://github.com/Dao-AILab/flash-attention\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:2020年4月16号三亚下雨么？\n",
      "A:{\"city\": \"三亚\", \"date\": \"2020-04-16\"}\n",
      "\n",
      "Q:青岛3-15号天气预报\n",
      "A:{\"city\": \"青岛\", \"date\": \"03-15\"}\n",
      "\n",
      "Q:5月6号下雪么，城市是威海\n",
      "A:{\"city\": \"威海\", \"date\": \"05-06\"}\n",
      "\n",
      "Q:青岛2023年12月30号有雾霾么?\n",
      "A:{\"city\": \"青岛\", \"date\": \"2023-12-30\"}\n",
      "\n",
      "Q:我打算6月1号去北京旅游，请问天气怎么样？\n",
      "A:{\"city\": \"北京\", \"date\": \"06-01\"}\n",
      "\n",
      "Q:你们打算1月3号坐哪一趟航班去上海？\n",
      "A:{\"city\": \"上海\", \"date\": \"01-03\"}\n",
      "\n",
      "Q:小明和小红是8月8号在上海结婚么?\n",
      "A:{\"city\": \"上海\", \"date\": \"08-08\"}\n",
      "\n",
      "Q:一起去东北看冰雕么，大概是1月15号左右，我们3个人一起\n",
      "A:{\"city\": \"东北\", \"date\": \"01-15\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer, GPTQConfig, AutoModelForCausalLM\n",
    "\n",
    "# ========== 1. 定义GPTQ量化配置（匹配微调时的模型量化方式） ==========\n",
    "gptq_config = GPTQConfig(\n",
    "    bits=4,  # 你的模型是4bit量化\n",
    "    disable_exllama=True,  # 适配旧版transformers，避免冲突\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# ========== 2. 加载Peft模型（兼容QuantLinear） ==========\n",
    "# 方式1：直接加载（推荐，自动合并LoRA适配器）\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    '../Qwen-main/output_qwen',  # Peft模型目录\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=gptq_config,  # 关键：指定量化配置，识别QuantLinear\n",
    "    low_cpu_mem_usage=True  # 减少CPU内存占用，适配量化模型\n",
    ").eval()\n",
    "\n",
    "model.generation_config.top_p=0 # 只选择概率最高的token\n",
    "\n",
    "Q_list=['2020年4月16号三亚下雨么？','青岛3-15号天气预报','5月6号下雪么，城市是威海','青岛2023年12月30号有雾霾么?','我打算6月1号去北京旅游，请问天气怎么样？','你们打算1月3号坐哪一趟航班去上海？','小明和小红是8月8号在上海结婚么?',\n",
    "        '一起去东北看冰雕么，大概是1月15号左右，我们3个人一起']\n",
    "#for Q in Q_list:\n",
    "#    prompt=prompt_template%(Q,)\n",
    "#    A,hist=model.chat(tokenizer,prompt,history=None)\n",
    "#    print('Q:%s\\nA:%s\\n'%(Q,A))\n",
    "\n",
    "history=None\n",
    "while True:\n",
    "    Q = input(\"请输入：\").strip()\n",
    "    prompt=prompt_template%(Q,)\n",
    "    A, history = model.chat(tokenizer, prompt, history=history)\n",
    "    print(f\"回答：{A}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cde061d-f448-43e9-9c80-2c484f00f0b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
