{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2568669c-c126-4791-93f8-d070a5835bcf",
   "metadata": {},
   "source": [
    "# 安装步骤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27090561-03ac-471e-acb0-c7dbeb373e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib2to3.pgen2.grammar import line\n",
    "\n",
    "linux控制台上使用root安装登录jupyter\n",
    "\n",
    "\n",
    "# 更新apt源，输入你的sudo密码\n",
    "sudo apt update && sudo apt upgrade -y\n",
    "\n",
    "# 安装Python3、pip3和必要依赖（Ubuntu默认没有预装pip）\n",
    "sudo apt install -y python3 python3-pip python3-dev\n",
    "\n",
    "# 升级pip3，加--user避免权限问题\n",
    "pip3 install --upgrade pip\n",
    "\n",
    "# 安装JupyterLab\n",
    "pip3 install jupyterlab\n",
    "# （可选）如果需要支持扩展插件，安装扩展管理器\n",
    "pip3 install jupyterlab_widgets\n",
    "\n",
    "jupyter lab --version\n",
    "\n",
    "# 1. 获取 jupyter 可执行文件路径\n",
    "which jupyter\n",
    "# 输出示例：/home/你的用户名/.local/bin/jupyter\n",
    "\n",
    "#2.创建并编辑服务文件：\n",
    "sudo vim /etc/systemd/system/jupyter-lab.service\n",
    "<<<\n",
    "[Unit]\n",
    "Description=JupyterLab Service (Root User)\n",
    "After=network.target\n",
    "\n",
    "[Service]\n",
    "Type=simple\n",
    "User=root\n",
    "ExecStart=/usr/local/bin/jupyter lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root\n",
    "WorkingDirectory=/home/wanglei/pyenv\n",
    "Restart=always\n",
    "RestartSec=10\n",
    "StandardOutput=append:/var/log/jupyter-lab.log\n",
    "StandardError=append:/var/log/jupyter-lab.err.log\n",
    "LimitNOFILE=65535\n",
    "\n",
    "[Install]\n",
    "WantedBy=multi-user.target\n",
    ">>>\n",
    "\n",
    "# 重载systemd配置（让修改生效）\n",
    "sudo systemctl daemon-reload\n",
    "\n",
    "# 重启服务\n",
    "sudo systemctl restart jupyter-lab.service\n",
    "\n",
    "# 查看服务状态（此时应显示active (running)）\n",
    "sudo systemctl status jupyter-lab.service\n",
    "\n",
    "# 检查8888端口\n",
    "sudo netstat -tlnp | grep 8888\n",
    "\n",
    "#执行密码设置命令\n",
    "# 用root权限执行（和服务运行用户一致）\n",
    "sudo jupyter server password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7137128b-9a21-4b60-8f02-ab94cd91a00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#一、全量卸载（系统级 + Python 包，彻底清空）\n",
    "#1. 卸载系统级 CUDA Toolkit（若已安装，全量清理）\n",
    "#适用于 Linux 系统（Ubuntu/CentOS 通用），彻底卸载所有 CUDA 版本：\n",
    "\n",
    "# 停止CUDA相关服务（如有）\n",
    "sudo systemctl stop nvidia-persistenced 2>/dev/null\n",
    "\n",
    "# 卸载CUDA Toolkit（覆盖12.x所有版本）\n",
    "sudo apt-get purge -y nvidia* cuda* cudnn* 2>/dev/null\n",
    "sudo apt-get autoremove -y\n",
    "sudo rm -rf /usr/local/cuda* /usr/lib/cuda* /usr/include/cuda*\n",
    "sudo rm -rf ~/.nv ~/.cuda\n",
    "\n",
    "# 清理CUDA环境变量（避免残留）\n",
    "sed -i '/CUDA_HOME/d' ~/.bashrc\n",
    "sed -i '/cuda/d' ~/.bashrc\n",
    "sed -i '/NVIDIA/d' ~/.bashrc\n",
    "source ~/.bashrc\n",
    "\n",
    "#2. 卸载所有 Python 依赖（全量清理）\n",
    "pip3 uninstall -y \\\n",
    "    torch torchvision torchaudio \\\n",
    "    transformers auto-gptq optimum peft \\\n",
    "    modelscope pyarrow numpy bitsandbytes triton datasets \\\n",
    "    accelerate sentencepiece safetensors\n",
    "\n",
    "# 清理pip缓存+残留文件\n",
    "pip3 cache purge\n",
    "rm -rf ~/.cache/huggingface ~/.cache/modelscope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3316012-ce58-4b0b-bca2-024b8490c5af",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#步骤 1：安装适配的 CUDA Toolkit 12.1\n",
    "#####cuda版本需注意安装12.1版本，按照你的显卡、cuda实际版本参考，尽量选择稳定版本\n",
    "nvidia-smi #查看当前硬件版本和cuda驱动\n",
    "nvcc -V  # 输出CUDA Version 12.1即正确\n",
    "\n",
    "# 因我的环境是cuda是12.8，在选择量化时报错bitsandbytes 预编译包、triton 依赖不兼容，所以需要卸载CUDA12.8，换成12.1\n",
    "# 1. 安装CUDA 12.1依赖\n",
    "sudo apt-get update && sudo apt-get install -y \\\n",
    "    gcc g++ make linux-headers-$(uname -r)\n",
    "\n",
    "# 2. 下载并安装CUDA 12.1（Ubuntu 20.04/22.04通用）\n",
    "wget https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda_12.1.0_530.30.02_linux.run\n",
    "sudo sh cuda_12.1.0_530.30.02_linux.run --silent --toolkit\n",
    "rm -f cuda_12.1.0_530.30.02_linux.run\n",
    "\n",
    "# 3. 配置CUDA环境变量（生效当前会话+永久生效）\n",
    "echo \"export CUDA_HOME=/usr/local/cuda-12.1\" >> ~/.bashrc\n",
    "echo \"export PATH=\\$CUDA_HOME/bin:\\$PATH\" >> ~/.bashrc\n",
    "echo \"export LD_LIBRARY_PATH=\\$CUDA_HOME/lib64:\\$LD_LIBRARY_PATH\" >> ~/.bashrc\n",
    "source ~/.bashrc\n",
    "\n",
    "# 验证CUDA安装\n",
    "nvcc -V  # 应输出 CUDA Version 12.1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d75e94-30b7-41cc-a7d0-f07e104a4184",
   "metadata": {},
   "outputs": [],
   "source": [
    "#步骤 2：安装 PyTorch（基础依赖，先装核心框架）\n",
    "#严格按官网量化要求选torch==2.1，且匹配 CUDA 12.1\n",
    "#####pytorch则需要按照cuda的版本进行选择安装，可参考官网 https://pytorch.org/get-started/locally/\n",
    "# CUDA 12.1\n",
    "pip install torch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "#此处安装可能会有警告，可以跳过"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aff5648-6819-4c7f-a6c1-895e47eec14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#步骤 3：安装量化相关依赖\n",
    "# 1. 先装基础依赖（numpy/pyarrow，避免量化包安装时依赖缺失）\n",
    "pip3 install numpy==1.26.4 pyarrow==14.0.1 sentencepiece safetensors\n",
    "\n",
    "# 2. 装官网要求的量化核心包（严格匹配版本）\n",
    "pip3 install \\\n",
    "transformers==4.34.1 \\\n",
    "auto-gptq==0.5.1 \\\n",
    "optimum==1.14.0 \\\n",
    "peft==0.6.2 \\\n",
    "modelscope==1.14.0 \\\n",
    "accelerate==0.24.1 \\\n",
    "huggingface_hub==0.19.4 \\\n",
    "deepspeed==0.14.0\n",
    "openmpi-bin openmpi-common libopenmpi-dev  mpi4py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "face27a3-c822-4d58-a8ba-794edd8924d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#linux控制台上qwen1.8B 安装\n",
    "#参考地址：https://github.com/QwenLM/Qwen/blob/main/README_CN.md\n",
    "\n",
    "# 1. 安装git（如果未安装）\n",
    "sudo apt install -y git\n",
    "\n",
    "# 2. 克隆Qwen官方仓库（包含requirements.txt）\n",
    "cd /home/wanglei/pyenv\n",
    "git clone https://github.com/QwenLM/Qwen.git\n",
    "cd Qwen/\n",
    "\n",
    "#优选：安装transformers指定版本：4.32.0 （需配套）\n",
    "/usr/bin/pip3 install transformers=4.32.0\n",
    "    \n",
    "#安装版本其他依赖\n",
    "pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d9eb8c-2ed5-4b42-94b0-83a2c8de0c0e",
   "metadata": {},
   "source": [
    "# 检查步骤 python代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a06a34f5-6fe7-499c-8353-1455efe7e629",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-25T05:27:42.121679665Z",
     "start_time": "2026-01-25T05:27:42.000492097Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.12\n"
     ]
    }
   ],
   "source": [
    "# 1. 检查Python版本\n",
    "!python3 --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa69ca65-aea2-4249-b84c-9df506a15867",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-25T05:27:45.691967067Z",
     "start_time": "2026-01-25T05:27:45.646420794Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter内核Python版本：3.10.12\n"
     ]
    }
   ],
   "source": [
    "# 2. 检查Jupyter内核Python版本\n",
    "import sys\n",
    "print(f\"Jupyter内核Python版本：{sys.version.split()[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff09e7f9-0a8e-4da3-b278-b0053d2cbad9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-25T05:27:49.222844778Z",
     "start_time": "2026-01-25T05:27:48.218805630Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch版本：2.1.2+cu121\n"
     ]
    }
   ],
   "source": [
    "# 3. 检查PyTorch版本\n",
    "import torch\n",
    "print(f\"PyTorch版本：{torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e6008ea-a2d0-471f-a0a5-332193cceb7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-25T05:27:55.227602193Z",
     "start_time": "2026-01-25T05:27:55.002761287Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers版本：4.35.2\n"
     ]
    }
   ],
   "source": [
    "# 4. 检查Transformers版本\n",
    "import transformers\n",
    "print(f\"Transformers版本：{transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de436c6c-fcfd-45c4-a50a-c4d7b2edc65c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-25T05:27:57.800540417Z",
     "start_time": "2026-01-25T05:27:57.739827893Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA是否可用：True\n",
      "CUDA版本（PyTorch检测）：12.1\n"
     ]
    }
   ],
   "source": [
    "# 5. 检查CUDA是否可用\n",
    "import torch\n",
    "print(f\"CUDA是否可用：{torch.cuda.is_available()}\")\n",
    "print(f\"CUDA版本（PyTorch检测）：{torch.version.cuda if torch.cuda.is_available() else '未安装CUDA'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f9c0dcc-36ca-42cc-8351-610da9ee3dad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-25T05:28:02.444987807Z",
     "start_time": "2026-01-25T05:28:01.824508402Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: 行 1: nvcc: 未找到命令\r\n",
      "Sun Jan 25 13:28:02 2026       \r\n",
      "+-----------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 570.172.18             Driver Version: 570.172.18     CUDA Version: 12.8     |\r\n",
      "|-----------------------------------------+------------------------+----------------------+\r\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                        |               MIG M. |\r\n",
      "|=========================================+========================+======================|\r\n",
      "|   0  NVIDIA RTX5880-Ada-8Q          Off |   00000000:00:09.0 Off |                  N/A |\r\n",
      "| N/A   N/A    P0            N/A  /  N/A  |    1758MiB /   8192MiB |      0%      Default |\r\n",
      "|                                         |                        |                  N/A |\r\n",
      "+-----------------------------------------+------------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+-----------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                              |\r\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\r\n",
      "|        ID   ID                                                               Usage      |\r\n",
      "|=========================================================================================|\r\n",
      "|    0   N/A  N/A            1620      G   /usr/bin/gnome-shell                    230MiB |\r\n",
      "|    0   N/A  N/A            2194      G   ...ersion=20251219-070015.367000        343MiB |\r\n",
      "+-----------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "# 6. 检查系统CUDA版本\n",
    "!nvcc -V  # 查看已安装的CUDA Toolkit版本\n",
    "!nvidia-smi  # 查看显卡驱动支持的最高CUDA版本"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c839902e-bdc2-454b-bf77-9fb8652dcdaa",
   "metadata": {},
   "source": [
    "# 加载 Qwen-1.8B-Chat-Int4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9757726d-c40b-414a-b369-7ffd9223b6c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-25T06:46:15.495820637Z",
     "start_time": "2026-01-25T06:41:01.490920547Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-25 14:41:01,777 - modelscope - INFO - Use user-specified model revision: v1.0.0\n",
      "Downloading: 100%|██████████| 8.21k/8.21k [00:00<00:00, 27.2MB/s]\n",
      "Downloading: 100%|██████████| 50.8k/50.8k [00:00<00:00, 24.7MB/s]\n",
      "Downloading: 100%|██████████| 244k/244k [00:00<00:00, 18.9MB/s]\n",
      "Downloading: 100%|██████████| 135k/135k [00:00<00:00, 17.9MB/s]\n",
      "Downloading: 100%|██████████| 1.17k/1.17k [00:00<00:00, 4.45MB/s]\n",
      "Downloading: 100%|██████████| 77.0/77.0 [00:00<00:00, 539kB/s]\n",
      "Downloading: 100%|██████████| 2.29k/2.29k [00:00<00:00, 18.5MB/s]\n",
      "Downloading: 100%|██████████| 1.88k/1.88k [00:00<00:00, 15.6MB/s]\n",
      "Downloading: 100%|██████████| 249/249 [00:00<00:00, 1.77MB/s]\n",
      "Downloading: 100%|██████████| 1.63M/1.63M [00:00<00:00, 59.0MB/s]\n",
      "Downloading: 100%|██████████| 1.84M/1.84M [00:00<00:00, 56.4MB/s]\n",
      "Downloading: 100%|██████████| 2.64M/2.64M [00:00<00:00, 66.2MB/s]\n",
      "Downloading: 100%|██████████| 7.11k/7.11k [00:00<00:00, 19.5MB/s]\n",
      "Downloading: 100%|██████████| 80.8k/80.8k [00:00<00:00, 19.7MB/s]\n",
      "Downloading: 100%|██████████| 80.8k/80.8k [00:00<00:00, 20.4MB/s]\n",
      "Downloading: 100%|█████████▉| 1.75G/1.75G [04:54<00:00, 6.38MB/s]\n",
      "Downloading: 100%|██████████| 54.3k/54.3k [00:00<00:00, 36.4MB/s]\n",
      "Downloading: 100%|██████████| 15.0k/15.0k [00:00<00:00, 37.3MB/s]\n",
      "Downloading: 100%|██████████| 237k/237k [00:00<00:00, 21.8MB/s]\n",
      "Downloading: 100%|██████████| 116k/116k [00:00<00:00, 16.0MB/s]\n",
      "Downloading: 100%|██████████| 215/215 [00:00<00:00, 1.53MB/s]\n",
      "Downloading: 100%|██████████| 2.44M/2.44M [00:00<00:00, 65.5MB/s]\n",
      "Downloading: 100%|██████████| 473k/473k [00:00<00:00, 25.9MB/s]\n",
      "Downloading: 100%|██████████| 14.3k/14.3k [00:00<00:00, 61.3MB/s]\n",
      "Downloading: 100%|██████████| 79.0k/79.0k [00:00<00:00, 16.3MB/s]\n",
      "Downloading: 100%|██████████| 46.4k/46.4k [00:00<00:00, 28.0MB/s]\n",
      "Downloading: 100%|██████████| 0.98M/0.98M [00:00<00:00, 22.4MB/s]\n",
      "Downloading: 100%|██████████| 205k/205k [00:00<00:00, 6.63MB/s]\n",
      "Downloading: 100%|██████████| 19.4k/19.4k [00:00<00:00, 3.03MB/s]\n",
      "Downloading: 100%|██████████| 302k/302k [00:00<00:00, 21.4MB/s]\n",
      "Downloading: 100%|██████████| 615k/615k [00:00<00:00, 38.1MB/s]\n",
      "Downloading: 100%|██████████| 376k/376k [00:00<00:00, 29.1MB/s]\n",
      "Downloading: 100%|██████████| 445k/445k [00:00<00:00, 26.5MB/s]\n",
      "Downloading: 100%|██████████| 23.8k/23.8k [00:00<00:00, 4.14MB/s]\n",
      "Downloading: 100%|██████████| 395k/395k [00:00<00:00, 25.6MB/s]\n",
      "Downloading: 100%|██████████| 176k/176k [00:00<00:00, 18.5MB/s]\n",
      "Downloading: 100%|██████████| 182k/182k [00:00<00:00, 18.3MB/s]\n",
      "Downloading: 100%|██████████| 824k/824k [00:00<00:00, 35.5MB/s]\n",
      "Downloading: 100%|██████████| 426k/426k [00:00<00:00, 25.8MB/s]\n",
      "Downloading: 100%|██████████| 433k/433k [00:00<00:00, 33.0MB/s]\n",
      "Downloading: 100%|██████████| 466k/466k [00:00<00:00, 25.6MB/s]\n",
      "Downloading: 100%|██████████| 403k/403k [00:00<00:00, 30.6MB/s]\n",
      "Downloading: 100%|██████████| 9.39k/9.39k [00:00<00:00, 22.3MB/s]\n",
      "Downloading: 100%|██████████| 403k/403k [00:00<00:00, 14.3MB/s]\n",
      "Downloading: 100%|██████████| 79.0k/79.0k [00:00<00:00, 21.2MB/s]\n",
      "Downloading: 100%|██████████| 173/173 [00:00<00:00, 1.48MB/s]\n",
      "Downloading: 100%|██████████| 41.9k/41.9k [00:00<00:00, 3.57MB/s]\n",
      "Downloading: 100%|██████████| 230k/230k [00:00<00:00, 21.9MB/s]\n",
      "Downloading: 100%|██████████| 1.27M/1.27M [00:00<00:00, 53.9MB/s]\n",
      "Downloading: 100%|██████████| 664k/664k [00:00<00:00, 34.3MB/s]\n",
      "Downloading: 100%|██████████| 404k/404k [00:00<00:00, 36.2MB/s]\n",
      "Try importing flash-attention for faster inference...\n",
      "Warning: import flash_attn rotary fail, please install FlashAttention rotary to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary\n",
      "Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm\n",
      "Warning: import flash_attn fail, please install FlashAttention to get higher efficiency https://github.com/Dao-AILab/flash-attention\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 模型回复：您好！我是一个由阿里云开发的AI模型，全名通义千问。我的身份是基于阿里云大模型系列在2019年发布的预训练语言模型“通义千问”。作为一个大型语言模型，通义千问具有非常强大的自然语言处理能力，可以回答各种问题、提供文本生成、聊天等任务，并能够根据用户的不同需求和场景提供个性化的服务。如果您有任何关于我的使用或者问题的问题，欢迎随时向我提问！\n",
      "✅ 模型运行设备：cuda:0\n"
     ]
    }
   ],
   "source": [
    "from modelscope import snapshot_download\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# 1. 下载量化模型\n",
    "model_dir = snapshot_download(\n",
    "    'qwen/Qwen-1_8B-Chat-Int4',\n",
    "    revision='v1.0.0',\n",
    "    cache_dir='../qwen_model'\n",
    ")\n",
    "\n",
    "# 2. 加载tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\n",
    "\n",
    "# 3. 关键：AutoModelForCausalLM加载量化模型（依赖AutoGPTQ）\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_dir,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ").eval()\n",
    "\n",
    "# 4. 测试推理\n",
    "response, history = model.chat(tokenizer, \"你好，请介绍一下自己\", history=None)\n",
    "print(f\"✅ 模型回复：{response}\")\n",
    "print(f\"✅ 模型运行设备：{next(model.parameters()).device}\")  # 输出cuda:0即GPU生效\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72106daa4b9ad056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a6a5cd0c1e09b8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-25T06:17:20.501698770Z",
     "start_time": "2026-01-25T06:17:20.132910799Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练数据生成完成！文件路径：train.json\n",
      "共生成 1000 条训练样本\n",
      "\n",
      "第一条样本示例：\n",
      "{\n",
      "  \"conversations\": [\n",
      "    {\n",
      "      \"from\": \"user\",\n",
      "      \"value\": \"\\n给定一句话：“12月12日镇宁布依族苗族自治县的天气”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\\\"city\\\":城市,\\\"date\\\":日期}\\n\\n请问，这个JSON字符串是：\\n\"\n",
      "    },\n",
      "    {\n",
      "      \"from\": \"assistant\",\n",
      "      \"value\": \"{\\\"city\\\": \\\"镇宁布依族苗族自治县\\\", \\\"date\\\": \\\"12-12\\\"}\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import json\n",
    "import time\n",
    "\n",
    "with open('./city.txt', 'r', encoding='utf-8') as f:\n",
    "    city_list = f.readlines()\n",
    "    city_list = [line.strip().split(' ')[1] for line in city_list]\n",
    "\n",
    "Q='青岛4月6日下雨么?'\n",
    "\n",
    "prompt_template='''\n",
    "给定一句话：“%s”，请你按步骤要求工作。\n",
    "\n",
    "步骤1：识别这句话中的城市和日期共2个信息\n",
    "步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\n",
    "\n",
    "请问，这个JSON字符串是：\n",
    "'''\n",
    "\n",
    "\n",
    "Q_arr = []\n",
    "A_arr = []\n",
    "\n",
    "Q_list = [\n",
    "    ('{city}{year}年{month}月{day}日的天气','%Y-%m-%d'),\n",
    "    ('{city}{year}年{month}月{day}号的天气','%Y-%m-%d'),\n",
    "    ('{city}{month}月{day}日的天气','%m-%d'),\n",
    "    ('{city}{month}月{day}号的天气','%m-%d'),\n",
    "\n",
    "    ('{year}年{month}月{day}日{city}的天气','%Y-%m-%d'),\n",
    "    ('{year}年{month}月{day}号{city}的天气','%Y-%m-%d'),\n",
    "    ('{month}月{day}日{city}的天气','%m-%d'),\n",
    "    ('{month}月{day}号{city}的天气','%m-%d'),\n",
    "\n",
    "    ('你们{year}年{month}月{day}日去{city}玩吗？','%Y-%m-%d'),\n",
    "    ('你们{year}年{month}月{day}号去{city}玩么？','%Y-%m-%d'),\n",
    "    ('你们{month}月{day}日去{city}玩吗？','%m-%d'),\n",
    "    ('你们{month}月{day}号去{city}玩吗？','%m-%d'),\n",
    "]\n",
    "\n",
    "# 生成1000条训练样本（日期转换/天气相关）\n",
    "for i in range(1000):\n",
    "    Q_template = Q_list[random.randint(0, len(Q_list)-1)]\n",
    "    city = city_list[random.randint(0, len(city_list)-1)]\n",
    "    year = random.randint(1990, 2025)\n",
    "    month = random.randint(1, 12)\n",
    "    day = random.randint(1, 28)\n",
    "    time_str = '{}-{}-{}'.format(year, month, day)\n",
    "    date_field = time.strftime(Q_template[1], time.strptime(time_str, '%Y-%m-%d'))\n",
    "    question = Q_template[0].format(city=city, year=year, month=month, day=day)  # 原始问题\n",
    "    answer = json.dumps({'city': city, 'date': date_field}, ensure_ascii=False)  # 回答\n",
    "\n",
    "    Q_arr.append(prompt_template % question)\n",
    "    A_arr.append(answer)\n",
    "\n",
    "# ========== 核心修改：生成conversations格式 ==========\n",
    "train_data = []\n",
    "for q, a in zip(Q_arr, A_arr):\n",
    "    train_data.append({\n",
    "        \"conversations\": [\n",
    "            {\"from\": \"user\", \"value\": q},       # 用户问题\n",
    "            {\"from\": \"assistant\", \"value\": a}   # 助手回答\n",
    "        ]\n",
    "    })\n",
    "\n",
    "# ========== 写入JSON文件（UTF-8编码，适配finetune脚本） ==========\n",
    "output_json_path = \"train.json\"  # 输出的训练文件路径\n",
    "with open(output_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(train_data, f, ensure_ascii=False, indent=2)  # indent=2方便查看，不影响训练\n",
    "\n",
    "print(f\"训练数据生成完成！文件路径：{output_json_path}\")\n",
    "print(f\"共生成 {len(train_data)} 条训练样本\")\n",
    "# 打印第一条样本示例，方便你核对格式\n",
    "print(\"\\n第一条样本示例：\")\n",
    "print(json.dumps(train_data[0], ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9948195af6819e84",
   "metadata": {},
   "source": [
    "# 微调模型，生成到output_qwen\n",
    "\n",
    "使用qwen提供的finetune脚本进行lora微调\n",
    "qlora支持量化微调\n",
    "ds 全参微调\n",
    "single 单卡\n",
    "\n",
    "脚本执行：\n",
    "Usage: bash finetune/finetune_qlora_ds.sh [-m MODEL_PATH] [-d DATA_PATH]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "231df1f30de5d19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bash: finetune/finetune_qlora_single_gpu.sh: 没有那个文件或目录\n"
     ]
    }
   ],
   "source": [
    "!cd /home/wanglei/pyenv/Qwen-main/\n",
    "!bash finetune/finetune_qlora_single_gpu.sh -m /home/wanglei/pyenv/qwen_model/qwen/Qwen-1_8B-Chat-Int4 -d /home/wanglei/pyenv/qwen3-1B-sft-learn/train.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818ab23b-55a7-4cb0-9e0c-cf78f14934ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
